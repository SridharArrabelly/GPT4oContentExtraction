{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Notebook to convert a document to Markdown format\n",
        "# Important - install requirements.txt as well as LibreOffice (via install-libreoffice.ipynb)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1718112447912
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil  \n",
        "import re  \n",
        "import json\n",
        "import time\n",
        "import requests\n",
        "import concurrent.futures  \n",
        "\n",
        "# For LibreOffice Doc Conversion to PDF\n",
        "import subprocess  \n",
        "import pathlib\n",
        "\n",
        "# Image extraction from PDF\n",
        "import fitz  # PyMuPDF  \n",
        "from pathlib import Path  \n",
        "import uuid\n",
        "\n",
        "# Image processing via GPT-4o  \n",
        "from openai import AzureOpenAI\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt \n",
        "import io\n",
        "import base64\n",
        "from IPython.display import Markdown, display  \n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1718310865333
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = json.load(open(\"config.json\"))\n",
        "\n",
        "# Azure AI Search Config\n",
        "search_service_name = config[\"search_service_name\"]\n",
        "search_service_url = \"https://{}.search.windows.net/\".format(search_service_name)\n",
        "\n",
        "search_admin_key = config[\"search_admin_key\"]\n",
        "\n",
        "index_name = config[\"search_index_name\"]\n",
        "index_schema_file = config[\"search_index_schema_file\"]\n",
        "search_api_version = config[\"search_api_version\"]\n",
        "search_headers = {  \n",
        "    'Content-Type': 'application/json',  \n",
        "    'api-key': search_admin_key  \n",
        "}  \n",
        "\n",
        "#Azure OpenAI\n",
        "openai_embedding_api_base = config[\"openai_embedding_api_base\"]\n",
        "openai_embedding_api_key = config[\"openai_embedding_api_key\"]\n",
        "openai_embedding_api_version = config[\"openai_embedding_api_version\"]\n",
        "openai_embeddings_model = config[\"openai_embedding_model\"]\n",
        "\n",
        "# gets the API Key from environment variable AZURE_OPENAI_API_KEY\n",
        "embeddings_client = AzureOpenAI(\n",
        "    api_version=openai_embedding_api_version,\n",
        "    azure_endpoint=openai_embedding_api_base,\n",
        "    api_key=openai_embedding_api_key\n",
        ")\n",
        "\n",
        "openai_gpt_api_base = config[\"openai_gpt_api_base\"]\n",
        "openai_gpt_api_key = config[\"openai_gpt_api_key\"]\n",
        "openai_gpt_api_version = config[\"openai_gpt_api_version\"]\n",
        "openai_gpt_model = config[\"openai_gpt_model\"]\n",
        "\n",
        "gpt_client = AzureOpenAI(\n",
        "    api_key=openai_gpt_api_key,  \n",
        "    api_version=openai_gpt_api_version,\n",
        "    base_url=f\"{openai_gpt_api_base}/openai/deployments/{openai_gpt_model}\"\n",
        ")\n",
        "\n",
        "supported_conversion_types = ['.pptx', '.ppt', '.docx', '.doc', '.xlsx', '.xls']\n",
        "\n",
        "print ('Search Service Name:', search_service_name)\n",
        "print ('Index Name:', index_name)\n",
        "print ('Azure OpenAI GPT Base URL:', openai_gpt_api_base)\n",
        "print ('Azure OpenAI GPT Model:', openai_gpt_model)\n",
        "print ('Azure OpenAI Embeddings Base URL:', openai_embedding_api_base)\n",
        "print ('Azure OpenAI Embeddings Model:', openai_embeddings_model)\n"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1718310870505
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directory if it does not exist\n",
        "def ensure_directory_exists(directory_path):  \n",
        "    path = Path(directory_path)  \n",
        "    if not path.exists():  \n",
        "        path.mkdir(parents=True, exist_ok=True)  \n",
        "        print(f\"Directory created: {directory_path}\")  \n",
        "    else:  \n",
        "        print(f\"Directory already exists: {directory_path}\")  \n",
        "  \n",
        "# Remove a dir and sub-dirs\n",
        "def remove_directory(directory_path):  \n",
        "    try:  \n",
        "        if os.path.exists(directory_path):  \n",
        "            shutil.rmtree(directory_path)  \n",
        "            print(f\"Directory '{directory_path}' has been removed successfully.\")  \n",
        "        else:  \n",
        "            print(f\"Directory '{directory_path}' does not exist.\")  \n",
        "    except Exception as e:  \n",
        "        print(f\"An error occurred while removing the directory: {e}\")  \n",
        "    \n",
        "# Convert to PDF\n",
        "def convert_to_pdf(input_path):  \n",
        "    # Ensure the output directory exists  \n",
        "    ensure_directory_exists('pdf')  \n",
        "\n",
        "    output_file = input_path.replace(pathlib.Path(input_path).suffix, '')\n",
        "    output_file = os.path.join('pdf', output_file + '.pdf')\n",
        "\n",
        "    if os.path.exists(output_file):\n",
        "        os.remove(output_file)\n",
        "      \n",
        "    # Command to convert pptx to pdf using LibreOffice  \n",
        "    command = [  \n",
        "        'soffice',  # or 'libreoffice' depending on your installation  \n",
        "        '--headless',  # Run LibreOffice in headless mode (no GUI)  \n",
        "        '--convert-to', 'pdf',  # Specify conversion format  \n",
        "        '--outdir', os.path.dirname(output_file),  # Output directory  \n",
        "        input_path  # Input file  \n",
        "    ]  \n",
        "      \n",
        "    # Run the command  \n",
        "    subprocess.run(command, check=True)  \n",
        "    print(f\"Conversion complete: {output_file}\")  \n",
        "    return output_file\n",
        "\n",
        "# Convert pages from PDF to images\n",
        "def extract_pdf_pages_to_images(pdf_path, image_dir):\n",
        "    # Validate image_out directory exists\n",
        "    doc_id = str(uuid.uuid4())\n",
        "    image_out_dir = os.path.join(image_dir, doc_id)\n",
        "    ensure_directory_exists(image_out_dir)  \n",
        "\n",
        "    # Open the PDF file and iterate pages\n",
        "    print ('Extracting images from PDF...')\n",
        "    pdf_document = fitz.open(pdf_path)  \n",
        "\n",
        "    for page_number in range(len(pdf_document)):  \n",
        "        page = pdf_document.load_page(page_number)  \n",
        "        image = page.get_pixmap()  \n",
        "        image_out_file = os.path.join(image_out_dir, f'{page_number + 1}.png')\n",
        "        image.save(image_out_file)  \n",
        "        if page_number % 100 == 0:\n",
        "            print(f'Processed {page_number} images...')  \n",
        "\n",
        "    return doc_id\n",
        "\n",
        "# Base64 encode images\n",
        "def encode_image(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "        \n",
        "# Find all files in a dir\n",
        "def get_all_files(directory_path):  \n",
        "    files = []  \n",
        "    for entry in os.listdir(directory_path):  \n",
        "        entry_path = os.path.join(directory_path, entry)  \n",
        "        if os.path.isfile(entry_path):  \n",
        "            files.append(entry_path)  \n",
        "    return files  \n",
        "  \n",
        "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
        "def extract_markdown_from_image(image_path):\n",
        "    base64_image = encode_image(image_path)\n",
        "    response = gpt_client.chat.completions.create(\n",
        "        model=openai_gpt_model,\n",
        "        messages=[\n",
        "            { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
        "            { \"role\": \"user\", \"content\": [  \n",
        "                { \n",
        "                    \"type\": \"text\", \n",
        "                    \"text\": \"\"\"Extract everything you see in this image to markdown. \n",
        "                        Convert all charts such as line, pie and bar charts to markdown tables and include a note that the numbers are approximate.\n",
        "                    \"\"\" \n",
        "                },\n",
        "                {\n",
        "                    \"type\": \"image_url\", \n",
        "                    \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"}\n",
        "                }\n",
        "            ] } \n",
        "        ],\n",
        "        max_tokens=2000 \n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def process_image(file):\n",
        "    if '.png' in file:\n",
        "        print ('Processing:', file)\n",
        "        markdown_text = extract_markdown_from_image(file)\n",
        "        markdown_file_out = os.path.join(markdown_out_dir, os.path.basename(file).replace('.png', '.txt'))\n",
        "        print(markdown_file_out)\n",
        "        with open(markdown_file_out, 'w') as md_out:\n",
        "            md_out.write(markdown_text)\n",
        "    else:\n",
        "        print ('Skipping non PNG file:', file)\n",
        "\n",
        "    return file\n",
        "  \n",
        "def extract_numeric_value(filename):  \n",
        "    # Extract numeric value from filename using regular expression  \n",
        "    match = re.search(r'(\\d+)', filename)  \n",
        "    return int(match.group(1)) if match else float('inf') \n",
        "    \n"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1718310874392
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_to_process = 'MicrosoftSlidesFY24Q3.pptx'\n"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1718310877001
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset the output directories\n",
        "if os.path.exists('json'):\n",
        "    remove_directory('json')\n",
        "if os.path.exists('images'):\n",
        "    remove_directory('images')\n",
        "if os.path.exists('markdown'):\n",
        "    remove_directory('markdown')\n",
        "if os.path.exists('pdf'):\n",
        "    remove_directory('pdf')\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Directory 'images' has been removed successfully.\nDirectory 'pdf' has been removed successfully.\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1718310880466
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If the file is not a PDF and can be converted - do so\n",
        "file_path = pathlib.Path(file_to_process).suffix\n",
        "ensure_directory_exists('pdf')\n",
        "\n",
        "if file_path in supported_conversion_types:\n",
        "    print ('Convering file to PDF...')\n",
        "    pdf_path = convert_to_pdf(file_to_process)\n",
        "else:\n",
        "    pdf_path = file_to_process\n",
        "    pdf_path = os.path.join('pdf', os.path.basename(file_to_process))\n",
        "    shutil.copy(file_to_process, pdf_path)  \n",
        "    print("File copied from", file_to_process, 'to', pdf_path) \n",
        "\n",
        "print ('PDF File to process:', pdf_path)\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Convering file to PDF...\nDirectory created: pdf\nconvert /mnt/batch/tasks/shared/LS_root/mounts/clusters/cpu-4core-32gb/code/Users/admin/test/insight-engine-customer-ready/MicrosoftSlidesFY24Q3.pptx -> /mnt/batch/tasks/shared/LS_root/mounts/clusters/cpu-4core-32gb/code/Users/admin/test/insight-engine-customer-ready/pdf/MicrosoftSlidesFY24Q3.pdf using filter : impress_pdf_Export\nConversion complete: pdf/MicrosoftSlidesFY24Q3.pdf\nPDF File to process: pdf/MicrosoftSlidesFY24Q3.pdf\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1718310888102
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract PDF pages to images\n",
        "image_path = 'images'\n",
        "\n",
        "doc_id = extract_pdf_pages_to_images(pdf_path, image_path)\n",
        "pdf_images_dir = os.path.join(image_path, doc_id)\n",
        "print ('Images saved to:', pdf_images_dir)\n",
        "  \n",
        "print ('Doc ID:', doc_id)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Directory created: images/6ba17e30-a320-49a5-9d44-c0f96a0e2869\nExtracting images from PDF...\nProcessed 0 images...\nImages saved to: images/6ba17e30-a320-49a5-9d44-c0f96a0e2869\nDoc ID: 6ba17e30-a320-49a5-9d44-c0f96a0e2869\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1718310890936
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "files = get_all_files(pdf_images_dir)  \n",
        "total_files = len(files)\n",
        "print ('Total Image Files to Process:', total_files)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Total Image Files to Process: 22\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1718310893610
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the images to markdown using GPT-4o \n",
        "# Process pages in parallel - adjust worker count as needed\n",
        "max_workers = 10\n",
        "\n",
        "markdown_out_dir = os.path.join('markdown', doc_id)\n",
        "ensure_directory_exists(markdown_out_dir)\n",
        "\n",
        "# Using ThreadPoolExecutor with a limit of max_workers threads  \n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:  \n",
        "    # Map the function to the array of items  \n",
        "    results = list(executor.map(process_image, files))  \n",
        "  \n",
        "print(results)  \n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Directory created: markdown/6ba17e30-a320-49a5-9d44-c0f96a0e2869\nProcessing: images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/1.png\nProcessing: images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/10.png\nProcessing: images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/11.png\nProcessing: images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/12.png\nProcessing: images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/13.png\nProcessing: images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/14.png\nProcessing: images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/15.png\nProcessing: images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/16.png\nProcessing: images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/17.png\nProcessing: images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/18.png\nmarkdown/6ba17e30-a320-49a5-9d44-c0f96a0e2869/15.txt\nProcessing: images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/19.png\nmarkdown/6ba17e30-a320-49a5-9d44-c0f96a0e2869/16.txt\nProcessing: images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/2.png\nmarkdown/6ba17e30-a320-49a5-9d44-c0f96a0e2869/1.txt\nProcessing: images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/20.png\nmarkdown/6ba17e30-a320-49a5-9d44-c0f96a0e2869/13.txt\nProcessing: images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/21.png\nmarkdown/6ba17e30-a320-49a5-9d44-c0f96a0e2869/18.txt\nmarkdown/6ba17e30-a320-49a5-9d44-c0f96a0e2869/2.txt\nProcessing: images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/22.png\nProcessing: images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/3.png\nmarkdown/6ba17e30-a320-49a5-9d44-c0f96a0e2869/14.txt\nProcessing: images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/4.png\nmarkdown/6ba17e30-a320-49a5-9d44-c0f96a0e2869/22.txt\nProcessing: images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/5.png\nmarkdown/6ba17e30-a320-49a5-9d44-c0f96a0e2869/11.txt\nProcessing: images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/6.png\nmarkdown/6ba17e30-a320-49a5-9d44-c0f96a0e2869/17.txt\nProcessing: images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/7.png\nmarkdown/6ba17e30-a320-49a5-9d44-c0f96a0e2869/3.txt\nProcessing: images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/8.png\nmarkdown/6ba17e30-a320-49a5-9d44-c0f96a0e2869/21.txt\nProcessing: images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/9.png\nmarkdown/6ba17e30-a320-49a5-9d44-c0f96a0e2869/10.txt\nmarkdown/6ba17e30-a320-49a5-9d44-c0f96a0e2869/12.txt\nmarkdown/6ba17e30-a320-49a5-9d44-c0f96a0e2869/4.txt\nmarkdown/6ba17e30-a320-49a5-9d44-c0f96a0e2869/19.txt\nmarkdown/6ba17e30-a320-49a5-9d44-c0f96a0e2869/6.txt\nmarkdown/6ba17e30-a320-49a5-9d44-c0f96a0e2869/7.txt\nmarkdown/6ba17e30-a320-49a5-9d44-c0f96a0e2869/20.txt\nmarkdown/6ba17e30-a320-49a5-9d44-c0f96a0e2869/9.txt\nmarkdown/6ba17e30-a320-49a5-9d44-c0f96a0e2869/8.txt\nmarkdown/6ba17e30-a320-49a5-9d44-c0f96a0e2869/5.txt\n['images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/1.png', 'images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/10.png', 'images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/11.png', 'images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/12.png', 'images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/13.png', 'images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/14.png', 'images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/15.png', 'images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/16.png', 'images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/17.png', 'images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/18.png', 'images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/19.png', 'images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/2.png', 'images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/20.png', 'images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/21.png', 'images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/22.png', 'images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/3.png', 'images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/4.png', 'images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/5.png', 'images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/6.png', 'images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/7.png', 'images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/8.png', 'images/6ba17e30-a320-49a5-9d44-c0f96a0e2869/9.png']\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1718310924522
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all the markdown files and sort them by page number\n",
        "files = os.listdir(markdown_out_dir)  \n",
        "\n",
        "# Filter out non-txt files (optional)  \n",
        "txt_files = [f for f in files if f.endswith('.txt')]  \n",
        "    \n",
        "# Sort files based on numeric values extracted from filenames  \n",
        "sorted_files = sorted(txt_files, key=extract_numeric_value)  \n",
        "\n",
        "total_files = len(sorted_files)\n",
        "print ('Total Markdown Files:', total_files)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Total Markdown Files: 22\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1718310924722
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = os.path.join(markdown_out_dir, sorted_files[7])  \n",
        "with open(file_path, 'r') as file:  \n",
        "    content = file.read()  \n",
        "    content = content.replace('```', '\\\\```')    \n",
        "    display(Markdown(content))  \n"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.Markdown object>",
            "text/markdown": "# Productivity and Business Processes\n## Overview\n\n### Investor Metrics\n\n|                        | FY23 Q3  | FY23 Q4  | FY24 Q1  | FY24 Q2  | FY24 Q3  |\n|------------------------|----------|----------|----------|----------|----------|\n| Office Commercial products and cloud services revenue growth (y/y) | 13% / 17% | 12% / 14% | 15% / 14% | 15% / 13% | 13% / 12% |\n| Office Consumer products and cloud services revenue growth (y/y) | 1% / 4%  | 3% / 6%  | 3% / 4%  | 5% / 4%  | 4%         |\n| Office 365 Commercial seat growth (y/y)                      | 11%      | 11%      | 10%      | 9%       | 8%         |\n| Microsoft 365 Consumer subscribers (in millions)             | 70.8     | 74.9     | 76.7     | 78.4     | 80.8       |\n| Dynamics products and cloud services revenue growth (y/y)    | 17% / 21% | 19% / 21% | 22% / 21% | 21% / 19% | 19% / 17% |\n| LinkedIn revenue growth (y/y)                               | 8% / 11% | 6% / 8%  | 8%       | 9% / 8%  | 10% / 9%   |\n\n*Growth rates include non-GAAP CC growth (GAAP % / CC %).*\n\n### Total Revenue\n\n- Revenue grew 12% (up 11% CC) driven by Office 365\n\n### Operating Income\n\n- Gross margin dollars grew 11% and gross margin percentage decreased slightly. Excluding the impact of the latest change in accounting estimate for useful lives, gross margin percentage increased slightly driven by improvement in Office 365.\n- Operating expenses grew 1% with investments in cloud engineering.\n- Operating income grew 17% (up 16% CC).\n\n### Total Revenue and Operating Income (FY23 Q3 to FY24 Q3)\n\n| FY23 Q3 | FY23 Q4 | FY24 Q1 | FY24 Q2 | FY24 Q3  |\n|---------|---------|---------|---------|----------|\n| $17.52 B / $8.64 B | $18.29 B / $9.05 B | $18.59 B / $9.97 B | $19.25 B / $10.28 B | $19.57 B +12% (+11% CC) / $10.14 B +17% (+16% CC) |\n\nNote: The numbers in the tables are approximate.\n\n*We have recast certain prior periods to conform to the way we internally manage and monitor our business. Includes non-GAAP constant currency (“CC”) growth. See Appendix for reconciliation of GAAP and non-GAAP measures. Growth rates in GAAP and CC are equivalent unless otherwise noted.*"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1718310924916
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
