{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Notebook to load markdown content to Azure AI Search\n",
        "# NOTE: There is no chunking of content, but rather a single document equals a PDF page"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1718310939971
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re  \n",
        "import json\n",
        "from pathlib import Path  \n",
        "\n",
        "import time\n",
        "import requests\n",
        "import concurrent.futures  \n",
        "\n",
        "from openai import AzureOpenAI\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt \n"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1718310955125
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = json.load(open(\"config.json\"))\n",
        "\n",
        "# Azure AI Search Config\n",
        "search_service_name = config[\"search_service_name\"]\n",
        "search_service_url = \"https://{}.search.windows.net/\".format(search_service_name)\n",
        "\n",
        "search_admin_key = config[\"search_admin_key\"]\n",
        "\n",
        "index_name = config[\"search_index_name\"]\n",
        "index_schema_file = config[\"search_index_schema_file\"]\n",
        "search_api_version = config[\"search_api_version\"]\n",
        "search_headers = {  \n",
        "    'Content-Type': 'application/json',  \n",
        "    'api-key': search_admin_key  \n",
        "}  \n",
        "\n",
        "#Azure OpenAI\n",
        "openai_embedding_api_base = config[\"openai_embedding_api_base\"]\n",
        "openai_embedding_api_key = config[\"openai_embedding_api_key\"]\n",
        "openai_embedding_api_version = config[\"openai_embedding_api_version\"]\n",
        "openai_embeddings_model = config[\"openai_embedding_model\"]\n",
        "\n",
        "# gets the API Key from environment variable AZURE_OPENAI_API_KEY\n",
        "embeddings_client = AzureOpenAI(\n",
        "    api_version=openai_embedding_api_version,\n",
        "    azure_endpoint=openai_embedding_api_base,\n",
        "    api_key=openai_embedding_api_key\n",
        ")\n",
        "\n",
        "openai_gpt_api_base = config[\"openai_gpt_api_base\"]\n",
        "openai_gpt_api_key = config[\"openai_gpt_api_key\"]\n",
        "openai_gpt_api_version = config[\"openai_gpt_api_version\"]\n",
        "openai_gpt_model = config[\"openai_gpt_model\"]\n",
        "\n",
        "gpt_client = AzureOpenAI(\n",
        "    api_key=openai_gpt_api_key,  \n",
        "    api_version=openai_gpt_api_version,\n",
        "    base_url=f\"{openai_gpt_api_base}/openai/deployments/{openai_gpt_model}\"\n",
        ")\n",
        "\n",
        "\n",
        "print ('Search Service Name:', search_service_name)\n",
        "print ('Index Name:', index_name)\n",
        "print ('Azure OpenAI GPT Base URL:', openai_gpt_api_base)\n",
        "print ('Azure OpenAI GPT Model:', openai_gpt_model)\n",
        "print ('Azure OpenAI Embeddings Base URL:', openai_embedding_api_base)\n",
        "print ('Azure OpenAI Embeddings Model:', openai_embeddings_model)\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1718310961545
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directory if it does not exist\n",
        "def ensure_directory_exists(directory_path):  \n",
        "    path = Path(directory_path)  \n",
        "    if not path.exists():  \n",
        "        path.mkdir(parents=True, exist_ok=True)  \n",
        "        print(f\"Directory created: {directory_path}\")  \n",
        "    else:  \n",
        "        print(f\"Directory already exists: {directory_path}\")  \n",
        "\n",
        "# Find all files in a dir\n",
        "def get_all_files(directory_path):  \n",
        "    files = []  \n",
        "    for entry in os.listdir(directory_path):  \n",
        "        entry_path = os.path.join(directory_path, entry)  \n",
        "        if os.path.isfile(entry_path):  \n",
        "            files.append(entry_path)  \n",
        "    return files  \n",
        "\n",
        "def extract_numeric_value(filename):  \n",
        "    # Extract numeric value from filename using regular expression  \n",
        "    match = re.search(r'(\\d+)', filename)  \n",
        "    return int(match.group(1)) if match else float('inf') \n",
        "        \n",
        "# Function to generate vectors for title and content fields, also used for query vectors\n",
        "max_attempts = 6\n",
        "max_backoff = 60\n",
        "@retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(max_attempts))\n",
        "def generate_embedding(text):\n",
        "    if text == None:\n",
        "        return None\n",
        "        \n",
        "    if len(text) < 10:\n",
        "        return None\n",
        "        \n",
        "    client = AzureOpenAI(\n",
        "        api_version=openai_embedding_api_version,\n",
        "        azure_endpoint=openai_embedding_api_base,\n",
        "        api_key=openai_embedding_api_key\n",
        "    )    \n",
        "    counter = 0\n",
        "    incremental_backoff = 1   # seconds to wait on throttline - this will be incremental backoff\n",
        "    while True and counter < max_attempts:\n",
        "        try:\n",
        "            # text-embedding-3-small == 1536 dims\n",
        "            response = client.embeddings.create(\n",
        "                input=text,\n",
        "                model=openai_embeddings_model\n",
        "            )\n",
        "            return json.loads(response.model_dump_json())[\"data\"][0]['embedding']\n",
        "        except openai.APIError as ex:\n",
        "            # Handlethrottling - code 429\n",
        "            if str(ex.code) == \"429\":\n",
        "                incremental_backoff = min(max_backoff, incremental_backoff * 1.5)\n",
        "                print ('Waiting to retry after', incremental_backoff, 'seconds...')\n",
        "                time.sleep(incremental_backoff)\n",
        "            elif str(ex.code) == \"content_filter\":\n",
        "                print ('API Error', ex.code)\n",
        "                return None\n",
        "        except Exception as ex:\n",
        "            counter += 1\n",
        "            print ('Error - Retry count:', counter, ex)\n",
        "    return None\n",
        "\n",
        "\n",
        "def process_json(file):\n",
        "    if '.json' in file:\n",
        "        print ('Processing:', file)\n",
        "        with open(file, 'r') as j_in:\n",
        "            json_data = json.loads(j_in.read())\n",
        "        json_data['vector'] = generate_embedding(json_data['content'])\n",
        "        with open(file, 'w') as j_out:\n",
        "            j_out.write(json.dumps(json_data, indent=4))\n",
        "    else:\n",
        "        print ('Skipping non JSON file:', file)\n",
        "\n",
        "    return file\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1718310963636
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate_embedding('That asd asdfasd asdf as df')"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1718310965284
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the doc_id\n",
        "folder_path = 'markdown'\n",
        "entries = os.listdir(folder_path)  \n",
        "directories = [entry for entry in entries if os.path.isdir(os.path.join(folder_path, entry))]  \n",
        "if len(directories) > 0:\n",
        "    doc_id = directories[0]\n",
        "    print ('Doc ID:', doc_id)\n",
        "else:\n",
        "    print ('Could not find the markdown files')\n",
        "\n",
        "markdown_out_dir = os.path.join('markdown', doc_id)    \n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Doc ID: 6ba17e30-a320-49a5-9d44-c0f96a0e2869\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1718310967471
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get all the markdown files and sort them by page number\n",
        "files = os.listdir(markdown_out_dir)  \n",
        "\n",
        "# Filter out non-txt files (optional)  \n",
        "txt_files = [f for f in files if f.endswith('.txt')]  \n",
        "    \n",
        "# Sort files based on numeric values extracted from filenames  \n",
        "sorted_files = sorted(txt_files, key=extract_numeric_value)  \n",
        "\n",
        "total_files = len(sorted_files)\n",
        "print ('Total Markdown Files:', total_files)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Total Markdown Files: 22\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1718310969290
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create JSON docs\n",
        "json_out_dir = os.path.join('json', doc_id)\n",
        "ensure_directory_exists(json_out_dir)\n",
        "\n",
        "counter = 0\n",
        "for c in files:\n",
        "    if '.txt' in c:\n",
        "        with open(os.path.join(markdown_out_dir, c), 'r') as c_in:\n",
        "            content = c_in.read()\n",
        "        # print (c)\n",
        "        # print (content)\n",
        "\n",
        "        json_data = {\n",
        "            'doc_id': doc_id, \n",
        "            'page_number': int(c.replace('page_', '').replace('.txt', '')),\n",
        "            'content': content\n",
        "            }\n",
        "\n",
        "        with open(os.path.join(json_out_dir, c.replace('.txt', '.json')), 'w') as c_out:\n",
        "            c_out.write(json.dumps(json_data, indent=4))\n",
        "\n",
        "        counter += 1\n",
        "        if counter % 100 == 0:\n",
        "            print (counter, 'json chunks written...')\n",
        "\n",
        "print (counter, 'json chunks written...')\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Directory created: json/6ba17e30-a320-49a5-9d44-c0f96a0e2869\n22 json chunks written...\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1718310978368
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the JSON Content\n",
        "json_files = get_all_files(json_out_dir)\n",
        "total_files = len(json_files)\n",
        "print ('Total JSON Files:', total_files)\n",
        "\n",
        "max_workers = 15\n",
        "\n",
        "# Using ThreadPoolExecutor with a limit of 10 threads  \n",
        "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:  \n",
        "    # Map the function to the array of items  \n",
        "    results = list(executor.map(process_json, json_files))  \n",
        "  \n",
        "print(results)  \n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Total JSON Files: 22\nProcessing: json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/1.json\nProcessing: json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/10.json\nProcessing: json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/11.json\nProcessing: json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/12.json\nProcessing: json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/13.json\nProcessing: json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/14.json\nProcessing: json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/15.json\nProcessing: json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/16.json\nProcessing: json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/17.json\nProcessing: json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/18.json\nProcessing: json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/19.json\nProcessing: json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/2.json\nProcessing: json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/20.json\nProcessing: json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/21.json\nProcessing: json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/22.json\nProcessing: json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/3.json\nProcessing: json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/4.json\nProcessing: json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/5.json\nProcessing: json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/6.json\nProcessing: json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/7.json\nProcessing: json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/8.json\nProcessing: json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/9.json\n['json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/1.json', 'json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/10.json', 'json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/11.json', 'json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/12.json', 'json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/13.json', 'json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/14.json', 'json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/15.json', 'json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/16.json', 'json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/17.json', 'json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/18.json', 'json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/19.json', 'json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/2.json', 'json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/20.json', 'json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/21.json', 'json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/22.json', 'json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/3.json', 'json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/4.json', 'json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/5.json', 'json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/6.json', 'json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/7.json', 'json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/8.json', 'json/6ba17e30-a320-49a5-9d44-c0f96a0e2869/9.json']\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1718310983896
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Index Scheam and ReCreate Index\n",
        "# Using REST for this to leverage most recent vector capabilities\n",
        "with open(index_schema_file, \"r\") as f_in:\n",
        "    index_schema = json.loads(f_in.read())\n",
        "    index_schema['name'] = index_name\n",
        "    index_schema['vectorSearch']['vectorizers'][0]['azureOpenAIParameters']['resourceUri'] = openai_embedding_api_base\n",
        "    index_schema['vectorSearch']['vectorizers'][0]['azureOpenAIParameters']['deploymentId'] = openai_embeddings_model\n",
        "    index_schema['vectorSearch']['vectorizers'][0]['azureOpenAIParameters']['apiKey'] = openai_embedding_api_key\n",
        "\n",
        "# Making the POST requests to re-create the index  \n",
        "delete_url = f\"{search_service_url}/indexes/{index_name}?api-version={search_api_version}\"  \n",
        "response = requests.delete(delete_url, headers=search_headers)  \n",
        "if response.status_code == 204:  \n",
        "    print(f\"Index {index_name} deleted successfully.\")  \n",
        "    # print(json.dumps(response.json(), indent=2))  \n",
        "else:  \n",
        "    print(\"Error deleting index, it may not exist.\")  \n",
        "\n",
        "# The endpoint URL for creating the index  \n",
        "create_index_url = f\"{search_service_url}/indexes?api-version={search_api_version}\"  \n",
        "response = requests.post(create_index_url, headers=search_headers, json=index_schema)  \n",
        "  \n",
        "# Check the response  \n",
        "if response.status_code == 201:  \n",
        "    print(f\"Index {index_name} created successfully.\")  \n",
        "    # print(json.dumps(response.json(), indent=2))  \n",
        "else:  \n",
        "    print(f\"Error creating index {index_name} :\")  \n",
        "    print(response.json())  \n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Index microsoft-results deleted successfully.\nIndex microsoft-results created successfully.\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1718310994585
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Index the content\n",
        "batch_size = 50\n",
        "index_doc_url = f\"{search_service_url}/indexes/{index_name}/docs/index?api-version={search_api_version}\" \n",
        "\n",
        "documents = {\"value\": []}\n",
        "for file in json_files:\n",
        "    if '.json' in file:\n",
        "        with open(file, 'r') as j_in:\n",
        "            json_data = json.loads(j_in.read())\n",
        "        json_data['doc_id'] = json_data['doc_id'] + '-' + str(json_data['page_number'])\n",
        "        documents[\"value\"].append(json_data)\n",
        "        if len(documents[\"value\"]) == batch_size:\n",
        "            response = requests.post(index_doc_url, headers=search_headers, json=documents)  \n",
        "            # Check the response  \n",
        "            if response.status_code == 200:  \n",
        "                print(f\"Document Indexed successfully.\")  \n",
        "                # print(json.dumps(response.json(), indent=2))  \n",
        "            else:  \n",
        "                print(f\"Error indexing document {file} :\")  \n",
        "                print(response.json())  \n",
        "            documents = {\"value\": []}\n",
        "            \n",
        "response = requests.post(index_doc_url, headers=search_headers, json=documents)  \n",
        "# Check the response  \n",
        "if response.status_code == 200:  \n",
        "    print(f\"Document Indexed successfully.\")  \n",
        "    # print(json.dumps(response.json(), indent=2))  \n",
        "else:  \n",
        "    print(f\"Error indexing document {file} :\")  \n",
        "    print(response.json())  \n",
        "documents = {\"value\": []}\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Document Indexed successfully.\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1718311000524
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1718119566690
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}